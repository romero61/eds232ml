---
title: "Lab6"
author: {Student Name}
date: "2023-03-01"
output: html_document
---

```{r}
library(tidyverse)
library(tidymodels)
library(readr)
library(skimr)
library(ggpubr)
library(patchwork)
library(caret)
library(corrplot)
library(flextable)
library(baguette)
library(ranger)
library(ggplot2)
library(vip)
library(here)
library(tictoc)
library(xgboost)
```

## Case Study Eel Species Distribution Modeling

This week's lab follows a modeling project described by Elith et al. (2008) (Supplementary Reading)

## Data

Grab the model training data set from the class Git:

data/eel.model.data.csv

```{r}
eel_data <- read_csv(here('eel.model.data.csv'))
```

```{r}
names(eel_data)
skim(eel_data)
histogram(eel_data$Angaus)
```

### Split and Resample

Split the joined data from above into a training and test set, stratified by outcome score. Use 10-fold CV to resample the training set, stratified by Angaus
```{r}
# Specify the outcome variable as a factor
eel_data$Angaus <- factor(eel_data$Angaus)
# Create training (70%) and test (30%) sets for the 
set.seed(123)  # for reproducibility (random sample)
data_split <- initial_split(eel_data, prop = 0.70, strata = Angaus)
data_train <- training(data_split)
data_test  <- testing(data_split)


# 10-fold CV on the training dataset
cv_folds <- data_train |>  vfold_cv(v = 10, strata = Angaus)

```

### Preprocess

Create a recipe to prepare your data for the XGBoost model. We are interested in predicting the binary outcome variable Angaus which indicates presence or absence of the eel species Anguilla australis

```{r}
# Create a recipe for the data
eel_recipe <- recipe(Angaus ~ ., data = data_train) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes())

# Print the recipe
eel_recipe
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined on Monday, first we conduct tuning on just the learn_rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()
```{r}
xgb_spec <- boost_tree(
  trees = 1000,  # A large number of trees to start with
  tree_depth = 4,  # Maximum depth of each tree
  min_n = 10,  # Minimum number of observations in each terminal node
  loss_reduction = 0,  # Minimum loss reduction required to make a further partition on a leaf node
  sample_size = 0.7,  # Fraction of observations to sample for each tree
  stop_iter = 10,  # Early stopping parameter
  # update the learning rate parameter to be tuned
  learn_rate = tune()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

```
2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))
```{r}
# Set up a grid for tuning the learning rate parameter
learn_rate_grid <- expand.grid(
  learn_rate = seq(0.0001, 0.3, length.out = 30)
)
```
-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().
```{r}
# Create a time tracker
tic()

# Tune the model using cross-validation
xgb_res <- tune_grid(
  xgb_spec,
  resamples = cv_folds,
  grid = learn_rate_grid,
  metrics = metric_set(roc_auc),
  control = control_grid(verbose = FALSE),
  preprocessor = eel_recipe
)

# Record the elapsed time
toc()
```

3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.
```{r}
xgb_res %>%
  show_best(metric = "roc_auc", n = 5)
```

### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.
```{r}
# Create a new specification with the optimal learning rate 0.01044138	roc_auc	binary	0.8378401	10	
xgb_spec2 <- boost_tree(
  trees = tune(),  # Tune the number of trees
  tree_depth = tune(),  # Tune the maximum depth of each tree
  min_n = tune(),  # Tune the minimum number of observations in each terminal node
  loss_reduction = tune(),  # Tune the minimum loss reduction required to make a further partition on a leaf node
  sample_size = 0.7,  # Fraction of observations to sample for each tree
  stop_iter = 10,  # Early stopping parameter
  learn_rate = 0.01044138  # Set the optimal learning rate
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```


2.  Set up a tuning grid. This time use grid_max_entropy() to get a representative sampling of the parameter space

```{r}
# Set up a grid to tune the tree parameters
tree_grid <- grid_max_entropy(
  trees(),
  tree_depth(),
  min_n(),
  loss_reduction(),
  size = 100
)

# Tune the model using cross-validation
xgb_res2 <- tune_grid(
  xgb_spec2,
  resamples = cv_folds,
  grid = tree_grid,
  metrics = metric_set(roc_auc),
  control = control_grid(verbose = FALSE),
  preprocessor = eel_recipe
)
```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.
```{r}
# Show the best models and their associated tree parameter values
best2 <- show_best(xgb_res2, "roc_auc", n = 10)
theme_box(flextable(best2) )

```

```{r}
theme_box(flextable(best2[1,]) )
```

### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}
# Create a new specification with the optimal learning rate and tree parameters
xgb_spec3 <- boost_tree(
  trees = 1420,  #  the number of trees
  tree_depth = 8,  #  the maximum depth of each tree
  min_n = 2,  # Tune the minimum number of observations in each terminal node
  loss_reduction = 0.00000339272,  #  the minimum loss reduction required to make a further partition on a leaf node
  sample_size = tune(),  # Fraction of observations to sample for each tree
  stop_iter = tune(),  # Early stopping parameter
  learn_rate = 0.01044138  # Set the optimal learning rate
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```


2.  Set up a tuning grid. Use grid_max_entropy() again.
```{r}
# Set up a grid to tune the stochastic parameters
stochastic_grid <- grid_max_entropy(
  sample_size = seq(0.1, 1, length.out = 10),
  stop_iter = seq(5, 50, length.out = 10)
)

# Tune the model using cross-validation
xgb_res2 <- tune_grid(
  xgb_spec3,
  resamples = cv_folds,
  grid = stochastic_grid ,
  metrics = metric_set(roc_auc),
  control = control_grid(verbose = FALSE),
  preprocessor = eel_recipe
)
```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

## Finalize workflow and make final prediction

1.  Assemble your final workflow will all of your optimized parameters and do a final fit.

2.  How well did your model perform? What types of errors did it make?

## Fit your model the evaluation data and compare performance

1.  Now fit your final model to the big dataset: data/eval.data.csv

2.  How does your model perform on this data?

3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?
